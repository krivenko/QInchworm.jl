{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "242ed4b2-c7e5-4821-97a8-a17f943f5659",
   "metadata": {},
   "source": [
    "# Fast tensor contraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb05c8-13f4-496e-94c0-5ac49d2af207",
   "metadata": {},
   "source": [
    "## Block-sparse tensor construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1979d6db-7703-481b-9b01-6d88ef327e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ITensors: Tensor, BlockSparseTensor, DiagBlockSparseTensor, Block, contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89f40c5-2280-494c-91fd-f018dfda1dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dim 1: [2, 2, 3]\n",
       "Dim 2: [4, 3]\n",
       "Dim 3: [3, 4]\n",
       "NDTensors.BlockSparse{ComplexF64, Vector{ComplexF64}, 3}\n",
       " 7×7×7\n",
       "Block(1, 1, 1)\n",
       " [1:2, 1:4, 1:3]\n",
       "[:, :, 1] =\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       "\n",
       "[:, :, 3] =\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       "\n",
       "Block(3, 2, 2)\n",
       " [5:7, 5:7, 4:7]\n",
       "[:, :, 1] =\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       "\n",
       "[:, :, 3] =\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       "\n",
       "[:, :, 4] =\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im\n",
       " 0.0 + 0.0im  0.0 + 0.0im  0.0 + 0.0im"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N = 3\n",
    "# 1st dimension => 3 blocks of sizes 2, 2, 3\n",
    "# 2nd dimension => 2 blocks of sizes 4, 3\n",
    "# 3rd dimension => 2 blocks of sizes 3, 4\n",
    "bst_dims = ([2, 2, 3], [4, 3], [3, 4])\n",
    "# Multi-indices of two non-vanishing blocks\n",
    "bst_blocks = [(1, 1, 1), (3, 2, 2)]\n",
    "\n",
    "# Construct a block-sparse tensor with zero-initialized memory\n",
    "bst = BlockSparseTensor(ComplexF64, bst_blocks, bst_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bdde05-a6f5-4dde-9151-1f2210d0fe30",
   "metadata": {},
   "source": [
    "## Access to individual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ece9c4a-2451-4b43-8a18-9b4a52b3918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bst = Dim 1: [2, 2, 3]\n",
      "Dim 2: [4, 3]\n",
      "Dim 3: [3, 4]\n",
      "NDTensors.BlockSparse{ComplexF64, Vector{ComplexF64}, 3}\n",
      " 7×7×7\n",
      "Block(1, 1, 1)\n",
      " [1:2, 1:4, 1:3]\n",
      "[:, :, 1] =\n",
      " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
      " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
      "\n",
      "[:, :, 2] =\n",
      " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
      " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
      "\n",
      "[:, :, 3] =\n",
      " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
      " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
      "\n",
      "Block(3, 2, 2)\n",
      " [5:7, 5:7, 4:7]\n",
      "[:, :, 1] =\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      "\n",
      "[:, :, 2] =\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      "\n",
      "[:, :, 3] =\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      "\n",
      "[:, :, 4] =\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
      " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dim 1: [2, 2, 3]\n",
       "Dim 2: [4, 3]\n",
       "Dim 3: [3, 4]\n",
       "NDTensors.BlockSparse{ComplexF64, Vector{ComplexF64}, 3}\n",
       " 7×7×7\n",
       "Block(1, 1, 1)\n",
       " [1:2, 1:4, 1:3]\n",
       "[:, :, 1] =\n",
       " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
       " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
       "\n",
       "[:, :, 2] =\n",
       " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
       " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
       "\n",
       "[:, :, 3] =\n",
       " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
       " 2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im  2.0 + 0.0im\n",
       "\n",
       "Block(3, 2, 2)\n",
       " [5:7, 5:7, 4:7]\n",
       "[:, :, 1] =\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       "\n",
       "[:, :, 2] =\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       "\n",
       "[:, :, 3] =\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       "\n",
       "[:, :, 4] =\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im\n",
       " 3.0 + 0.0im  3.0 + 0.0im  3.0 + 0.0im"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst[Block(1, 1, 1)] = 2.0;\n",
    "bst[Block(3, 2, 2)] = 3.0;\n",
    "\n",
    "@show bst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c91bf9-c180-4633-a403-1e32a208f6f4",
   "metadata": {},
   "source": [
    "## Test simple contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b74c49c-91f4-49ee-b3f8-40fe4372054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = BlockSparseTensor(ComplexF64, [(1, 1), (2, 2)], ([2, 2], [2, 2]));\n",
    "A11 = [1 2; 3 4];\n",
    "A22 = [5 6; 7 8];\n",
    "A[Block(1, 1)] = A11\n",
    "A[Block(2, 2)] = A22\n",
    "\n",
    "B = BlockSparseTensor(ComplexF64, [(1, 1), (2, 2)], ([2, 2], [2, 2]));\n",
    "B11 = [9 10; 11 12];\n",
    "B22 = [13 14; 15 16];\n",
    "B[Block(1, 1)] = B11;\n",
    "B[Block(2, 2)] = B22;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7f70f-87e7-468d-96e1-2644e7535b7a",
   "metadata": {},
   "source": [
    "Outer product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da795642-f47d-4f1c-8ab6-ac0d1fdb5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = contract(A, (1, 2), B, (3, 4), (1, 2, 3, 4))\n",
    "@assert res[Block(1, 1, 1, 1)] == [A11[i, j] * B11[k, l] for i=1:2, j=1:2, k=1:2, l=1:2]\n",
    "@assert res[Block(1, 1, 2, 2)] == [A11[i, j] * B22[k, l] for i=1:2, j=1:2, k=1:2, l=1:2]\n",
    "@assert res[Block(2, 2, 1, 1)] == [A22[i, j] * B11[k, l] for i=1:2, j=1:2, k=1:2, l=1:2]\n",
    "@assert res[Block(2, 2, 2, 2)] == [A22[i, j] * B22[k, l] for i=1:2, j=1:2, k=1:2, l=1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29a19e-5e16-47d2-9fd2-2fb5a7a0ffb8",
   "metadata": {},
   "source": [
    "Matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5d554c-63da-43b3-a6a6-3d5ec94e2be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = contract(A, (1, 2), B, (2, 3), (1, 3))\n",
    "@assert res[Block(1, 1)] == A11 * B11\n",
    "@assert res[Block(2, 2)] == A22 * B22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae433b20-ccac-4890-920f-acceb6172257",
   "metadata": {},
   "source": [
    "Trace of matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f6a894-6a67-4aea-8fbc-30db9213afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra: tr\n",
    "\n",
    "res = contract(A, (1, 2), B, (2, 1), ())\n",
    "@assert res[] == tr(A11 * B11) + tr(A22 * B22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb495f-9482-4d4e-bd7a-12b33337815c",
   "metadata": {},
   "source": [
    "## Contraction FLOPS cost and peak memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af585969-6b9b-4985-a2f0-177e16aa2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ITensors.NDTensors: inds,\n",
    "                          nzblocks,\n",
    "                          blockdims,\n",
    "                          contract_inds,\n",
    "                          contract_labels,\n",
    "                          contract_blocks,\n",
    "                          are_blocks_contracted,\n",
    "                          ValLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8161bcc1-50c0-4dee-b61a-35dc7709354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FLOPS cost of a single tensor pair contraction.\n",
    "\"\"\"\n",
    "function flops_cost(blocks1, inds1, labels1, blocks2, inds2, labels2, labelsR)\n",
    "    labels1_to_labels2, labels1_to_labelsR, labels2_to_labelsR = contract_labels(labels1, labels2, labelsR)\n",
    "    cost = 0\n",
    "    for block1 in blocks1\n",
    "        for block2 in blocks2\n",
    "            if are_blocks_contracted(block1, block2, labels1_to_labels2)\n",
    "                dims1 = blockdims(inds1, block1)\n",
    "                dims2 = blockdims(inds2, block2)\n",
    "                cost += prod(dims1) * prod(dims2[a2] for (a2, aR) in enumerate(labels2_to_labelsR) if aR != 0)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return cost\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Cost of a single tensor pair contraction.\n",
    "\"\"\"\n",
    "function flops_cost(T1::BlockSparseTensor, labels1, T2::BlockSparseTensor, labels2, labelsR)\n",
    "    return flops_cost(nzblocks(T1), inds(T1), labels1,\n",
    "                      nzblocks(T2), inds(T2), labels2,\n",
    "                      labelsR\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4421e6ad-ca94-4157-ab86-879b9afe41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Memory occupied by a tensor\"\"\"\n",
    "mem(blocks, inds) = sum(prod(blockdims(inds, b)) for b in blocks)\n",
    "\n",
    "\"\"\"\n",
    "Memory required to perform a single tensor pair contraction.\n",
    "\"\"\"\n",
    "function peak_memory(blocks1, inds1, labels1, blocks2, inds2, labels2, labelsR)\n",
    "    labels1_to_labels2, labels1_to_labelsR, labels2_to_labelsR = contract_labels(labels1, labels2, labelsR)\n",
    "    blocksR = []\n",
    "    for block1 in blocks1\n",
    "        for block2 in blocks2\n",
    "            if are_blocks_contracted(block1, block2, labels1_to_labels2)\n",
    "                push!(blocksR, contract_blocks(block1, labels1_to_labelsR, block2, labels2_to_labelsR, ValLength(labelsR)))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    indsR = contract_inds(inds1, labels1, inds2, labels2, labelsR)\n",
    "    return mem(blocks1, inds1) + mem(blocks2, inds2) + mem(blocksR, indsR)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Memory required to perform a single tensor pair contraction.\n",
    "\"\"\"\n",
    "function peak_memory(T1::BlockSparseTensor, labels1, T2::BlockSparseTensor, labels2, labelsR)\n",
    "    return peak_memory(nzblocks(T1), inds(T1), labels1,\n",
    "                       nzblocks(T2), inds(T2), labels2,\n",
    "                       labelsR\n",
    "    )\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07bde1-d228-4634-a3b6-0dcc686c86f8",
   "metadata": {},
   "source": [
    "Test `flops_cost()` and `peak_memory()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a86f7ce9-48c0-42dd-958d-fb96b21119fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = BlockSparseTensor(ComplexF64, [(1, 1, 2, 3, 1),\n",
    "                                    (2, 2, 3, 2, 1),\n",
    "                                    (3, 2, 1, 2, 1)],\n",
    "                                    ([2, 3, 4], [3, 2, 3], [4, 5, 2], [5, 1, 3], [6, 2, 7]));\n",
    "T2 = BlockSparseTensor(ComplexF64, [(1, 1, 1, 2, 2),\n",
    "                                    (2, 3, 1, 2, 1),\n",
    "                                    (1, 2, 3, 1, 1)],\n",
    "                                    ([10, 2, 3], [5, 1, 3], [2, 3, 4], [7, 4, 1], [8, 2, 2]));\n",
    "\n",
    "# Contraction over 2 labels, '1' and '4'\n",
    "labels1 = (1, 2, 3, 4, 5)\n",
    "labels2 = (6, 4, 1, 9, 10)\n",
    "labelsR = (2, 3, 5, 6, 10, 9)\n",
    "\n",
    "R = contract(T1, labels1, T2, labels2, labelsR)\n",
    "\n",
    "cost = flops_cost(T1, labels1, T2, labels2, labelsR)\n",
    "memory = peak_memory(T1, labels1, T2, labels2, labelsR)\n",
    "\n",
    "@assert cost == (2*3*5*3*6)*(2*4*8) + # Block(1, 1, 2, 3, 1) ∘ Block(2, 3, 1, 2, 1)\n",
    "                (4*2*4*1*6)*(10*7*8)  # Block(3, 2, 1, 2, 1) ∘ Block(1, 2, 3, 1, 1)\n",
    "@assert memory == length(T1.storage.data) + length(T2.storage.data) + length(R.storage.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648be49-69c1-4016-a27d-15ea5816c2a5",
   "metadata": {},
   "source": [
    "Binary tree of pairwise contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "877e0d49-0b63-42f8-a828-e7b71309da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using AbstractTrees\n",
    "\n",
    "mutable struct ContractionTree\n",
    "    \"Left tensor in pair contraction\"\n",
    "    left::Union{ContractionTree, Nothing}\n",
    "    \"Right tensor in pair contraction\"\n",
    "    right::Union{ContractionTree, Nothing}\n",
    "    \"Position of the tensor in the tensor network input list (valid only for leaves)\"\n",
    "    pos::Union{Int, Nothing}\n",
    "    \"List of non-zero blocks of the tensor\"\n",
    "    blocks::Vector{Block}\n",
    "    \"Block sizes of the tensor\"\n",
    "    inds::NTuple\n",
    "    \"Labels carried by the tensor\"\n",
    "    labels::NTuple\n",
    "end\n",
    "\n",
    "isleaf(tree::ContractionTree) = (tree.left === nothing) && (tree.right === nothing);\n",
    "AbstractTrees.children(tree::ContractionTree) = isleaf(tree) ? [] : [tree.left, tree.right];\n",
    "AbstractTrees.printnode(io::IO, tree::ContractionTree) = print(io, tree.labels);\n",
    "\n",
    "\"Make a leaf of a contraction tree\"\n",
    "ContractionTree(T, pos, labels) = ContractionTree(nothing, nothing, pos, nzblocks(T), inds(T), labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd9311-f10b-405c-a09a-6dd6cca115fb",
   "metadata": {},
   "source": [
    "Test the contraction tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c111dbf1-4c03-4b36-9782-13a58a99e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3)\n",
      "(3, 2, 4)\n",
      "(1, 4)\n",
      "├─ (1, 2, 3)\n",
      "└─ (3, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "let t1 = ContractionTree(nothing, nothing, 1, [], (5, 5, 5), (1, 2, 3)),\n",
    "    t2 = ContractionTree(nothing, nothing, 2, [], (5, 5, 5), (3, 2, 4)),\n",
    "    t12 = ContractionTree(t1, t2, nothing, [], (5, 5), (1, 4))\n",
    "    print_tree(t1)\n",
    "    print_tree(t2)\n",
    "    print_tree(t12)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afea109-0c52-40c5-8f5c-1237ae3bf01b",
   "metadata": {},
   "source": [
    "Cost of a tensor network contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5963a7e-a8ae-4eff-988b-6a10b6d4d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FLOPS cost of a tensor network contraction.\n",
    "\"\"\"\n",
    "function flops_cost(tree::ContractionTree)\n",
    "    isleaf(tree) && return 0\n",
    "\n",
    "    cost_left = flops_cost(tree.left)\n",
    "    cost_right = flops_cost(tree.right)\n",
    "    cost = flops_cost(tree.left.blocks, tree.left.inds, tree.left.labels,\n",
    "                      tree.right.blocks, tree.right.inds, tree.right.labels,\n",
    "                      tree.labels)\n",
    "    \n",
    "    return cost_left + cost_right + cost\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0afe6955-7921-424c-b699-27ba89394c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Peak memory required to store intermediate results of a tensor network contraction.\n",
    "\"\"\"\n",
    "function peak_memory(tree::ContractionTree)\n",
    "    isleaf(tree) && return 0\n",
    "\n",
    "    memory_left = peak_memory(tree.left)\n",
    "    memory_right = peak_memory(tree.right)\n",
    "    memory = peak_memory(tree.left.blocks, tree.left.inds, tree.left.labels,\n",
    "                         tree.right.blocks, tree.right.inds, tree.right.labels,\n",
    "                         tree.labels)\n",
    "\n",
    "    return max(memory_left, memory_right, memory)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1341e-49cc-46a0-a61f-c008a45f5c57",
   "metadata": {},
   "source": [
    "## Construct a tensor network describing a given $\\Sigma$ topology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b21072-2162-4e18-8d57-357a0c181cbe",
   "metadata": {},
   "source": [
    "`Topology` type from QInchworm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4962c243-b49c-4c3b-a5ec-31b373ac6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Combinatorics: levicivita\n",
    "\n",
    "const PairVector = Vector{Pair{Int,Int}}\n",
    "\n",
    "struct Topology\n",
    "    \"Topology order ``n``\"\n",
    "    order::Int\n",
    "    \"List of pairs ``\\\\{(\\\\pi(1), \\\\pi(2)), ..., (\\\\pi(2n-1), \\\\pi(2n))\\\\}``\"\n",
    "    pairs::PairVector\n",
    "    \"Parity of the permutation ``\\\\pi``\"\n",
    "    parity::Int\n",
    "\n",
    "    function Topology(pairs::PairVector, parity::Int)\n",
    "        return new(length(pairs), pairs, parity)\n",
    "    end\n",
    "end\n",
    "\n",
    "function Topology(pairs::PairVector)\n",
    "    p = levicivita(collect(Iterators.flatten(pairs)))\n",
    "    return Topology(pairs, p)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6febb-8ae5-4969-9af3-65f2030c8701",
   "metadata": {},
   "source": [
    "Construct a tensor network describing a PP self-energy diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e450e54-bf13-461e-8036-d93e28e1b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_Σ_tensor_network(top::Topology, P::Vector, O, pair_ints::Vector)\n",
    "    n = top.order\n",
    "    @assert length(P) == 2n - 1\n",
    "    @assert length(pair_ints) == n\n",
    "    \n",
    "    # First, define the PP backbone\n",
    "    # Labels of PP-indices are assigned in the contour order\n",
    "    # Labels of interaction indices are assigned in the contour order of the respective operators O\n",
    "    network = []\n",
    "    int_label = 1\n",
    "    for pos in 1:(4n - 1)\n",
    "        if isodd(pos)\n",
    "            # Add an interaction vertex\n",
    "            labels = (4n + int_label, pos + 1, pos)\n",
    "            push!(network, O => labels)\n",
    "            int_label += 1\n",
    "        else\n",
    "            # Add a PP propagator\n",
    "            labels = (pos + 1, pos)\n",
    "            push!(network, P[div(pos, 2)] => labels)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Translate positions of interaction arc ends and sort the arcs\n",
    "    # according to the order their heads appear in a configuration\n",
    "    arcs = sort([(2n + 1 - p[2], 2n + 1 - p[1]) for p in top.pairs], lt=(p1, p2) -> p1[1] < p2[1])\n",
    "    \n",
    "    # Add the interaction lines\n",
    "    for (pair_int, arc) in zip(pair_ints, arcs)\n",
    "        labels = (4n + arc[2], 4n + arc[1])\n",
    "        push!(network, pair_int => labels)\n",
    "    end\n",
    "    \n",
    "    return network\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72095ebf-2daf-41a6-bba0-a19991f6ad16",
   "metadata": {},
   "source": [
    "Construct a similar tensor network without PP propagators sandwiched between vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a4262dd0-6dfc-44f0-a22a-96255b3f5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_Σ_tensor_network_wo_P(top::Topology, O, pair_ints::Vector)\n",
    "    n = top.order\n",
    "    @assert length(P) == 2n - 1\n",
    "    @assert length(pair_ints) == n\n",
    "    \n",
    "    # First, define the PP backbone\n",
    "    # Labels of PP-indices are assigned in the contour order\n",
    "    # Labels of interaction indices are assigned in the contour order of the respective operators O\n",
    "    network = []\n",
    "    #int_label = 1\n",
    "    for pos in 1:2n\n",
    "        labels = (2n + 1 + pos, pos + 1, pos)\n",
    "        push!(network, O => labels)\n",
    "    end\n",
    "\n",
    "    # Translate positions of interaction arc ends and sort the arcs\n",
    "    # according to the order their heads appear in a configuration\n",
    "    arcs = sort([(2n + 1 - p[2], 2n + 1 - p[1]) for p in top.pairs], lt=(p1, p2) -> p1[1] < p2[1])\n",
    "    \n",
    "    # Add the interaction lines\n",
    "    for (pair_int, arc) in zip(pair_ints, arcs)\n",
    "        labels = (2n + 1 + arc[2], 2n + 1 + arc[1])\n",
    "        push!(network, pair_int => labels)\n",
    "    end\n",
    "    \n",
    "    return network\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6cc69-65a0-41f4-a7df-f21cd44e3df8",
   "metadata": {},
   "source": [
    "## Contraction of a chain tensor network ($n = 3$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b048154-d008-4612-863e-2190c4d89110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagators: 2 dimension, 3 square blocks per each dimension\n",
    "P_block_dims = ([2, 3, 4], [2, 3, 4])\n",
    "P_dims = sum.(P_block_dims)\n",
    "# The only non-zero blocks are diagonal\n",
    "P_blocks = [(i, i) for i in 1:length(first(P_block_dims))]\n",
    "\n",
    "P = [BlockSparseTensor(ComplexF64, P_blocks, P_block_dims) for i in 1:5];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2a0a2-1567-4ebf-a004-76c975f04e5e",
   "metadata": {},
   "source": [
    "Test contraction of propagators alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "17b987e2-ac1d-4378-b9c8-9d72b5b6dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random: MersenneTwister, rand\n",
    "rng = MersenneTwister(12345678)\n",
    "\n",
    "for i in 1:5\n",
    "    for b in 1:length(first(P_block_dims))\n",
    "        P[i][Block(b, b)] = rand(rng, P_block_dims[1][b], P_block_dims[2][b])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Labels are attached to propagators as follows:\n",
    "# P5_{6,5} P4_{5,4} P3_{4,3} P2_{3,2} P1_{2,1}\n",
    "\n",
    "# Test contraction order: (P5 * (P4 * P3)) * (P2 * P1)\n",
    "R43 = contract(P[4], (5, 4), P[3], (4, 3), (5, 3));\n",
    "R543 = contract(P[5], (6, 5), R43, (5, 3), (6, 3));\n",
    "R21 = contract(P[2], (3, 2), P[1], (2, 1), (3, 1));\n",
    "R = contract(R543, (6, 3), R21, (3, 1), (6, 1))\n",
    "\n",
    "for b in 1:length(first(P_block_dims))\n",
    "    bl = Block(b, b)\n",
    "    R_mat = convert(Matrix{ComplexF64}, R[bl])\n",
    "    R_mat_ref = convert(Matrix{ComplexF64}, P[5][bl]) *\n",
    "                convert(Matrix{ComplexF64}, P[4][bl]) *\n",
    "                convert(Matrix{ComplexF64}, P[3][bl]) *\n",
    "                convert(Matrix{ComplexF64}, P[2][bl]) *\n",
    "                convert(Matrix{ComplexF64}, P[1][bl])\n",
    "    @assert isapprox(R_mat, R_mat_ref)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7d12083c-0430-4532-b2dd-807f7db6822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction lines\n",
    "N_int = 6 # Number of pair interactions\n",
    "#Δ = [BlockSparseTensor(ComplexF64, [(n, n) for n in 1:N_int], (ones(Int, N_int), ones(Int, N_int))) for i=1:3];\n",
    "Δ = [DiagBlockSparseTensor(ComplexF64(i), [Block(1, 1)], ([N_int], [N_int])) for i=1:3];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d0a560d5-c7e2-4d47-aa8e-1d85b44f2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction vertices: 3 dimensions\n",
    "# There is always only one block along the 1st dimenstion (interaction index)\n",
    "O_block_dims = ([N_int], P_block_dims...)\n",
    "O_blocks = sort([(1, 1, 2),\n",
    "                 (1, 2, 1),\n",
    "                 (1, 2, 3),\n",
    "                 (1, 3, 2),\n",
    "                 (1, 1, 3),\n",
    "                 (1, 3, 1)])\n",
    "\n",
    "O = BlockSparseTensor(ComplexF64, O_blocks, O_block_dims);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a1d30-27a3-4e08-84dd-b0188e70d28c",
   "metadata": {},
   "source": [
    "Define tensor network corresponding to the $n=3$ chain diagram\n",
    "\n",
    "![Chain diagram n=3](chain_n3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d674c7-e2d4-442b-a5f9-36587f58a145",
   "metadata": {},
   "source": [
    "Test `make_Σ_tensor_network()` and `make_Σ_tensor_network_wo_P()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "295f94e2-7148-46ed-b5c6-947363d7d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Topology([1 => 3, 2 => 5, 4 => 6])\n",
    "\n",
    "tensor_network = make_Σ_tensor_network(top, P, O, Δ);\n",
    "tensor_network_ref = Any[\n",
    "    # PP-backbone\n",
    "    O    => (13, 2, 1),\n",
    "    P[1] => (3, 2),\n",
    "    O    => (14, 4, 3),\n",
    "    P[2] => (5, 4),\n",
    "    O    => (15, 6, 5),\n",
    "    P[3] => (7, 6),\n",
    "    O    => (16, 8, 7),\n",
    "    P[4] => (9, 8),\n",
    "    O    => (17, 10, 9),\n",
    "    P[5] => (11, 10),\n",
    "    O    => (18, 12, 11),\n",
    "    # Interaction lines\n",
    "    Δ[1] => (15, 13),\n",
    "    Δ[2] => (17, 14),\n",
    "    Δ[3] => (18, 16)\n",
    "];\n",
    "@assert tensor_network == tensor_network_ref\n",
    "\n",
    "tensor_network_wo_P = make_Σ_tensor_network_wo_P(top, O, Δ);\n",
    "tensor_network_wo_P_ref = Any[\n",
    "    # PP-backbone\n",
    "    O    => (8, 2, 1),\n",
    "    O    => (9, 3, 2),\n",
    "    O    => (10, 4, 3),\n",
    "    O    => (11, 5, 4),\n",
    "    O    => (12, 6, 5),\n",
    "    O    => (13, 7, 6),\n",
    "    # Interaction lines\n",
    "    Δ[1] => (10, 8),\n",
    "    Δ[2] => (12, 9),\n",
    "    Δ[3] => (13, 11)\n",
    "];\n",
    "@assert tensor_network_wo_P == tensor_network_wo_P_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84f08a-8b8b-41cb-97d0-5b7dc302421b",
   "metadata": {},
   "source": [
    "## Study various contraction orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a0cad6e7-c860-4e96-b8d8-1c1aca8a37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function contraction_tree_orig_order(network)::ContractionTree\n",
    "    n = div(length(network) + 1, 5)\n",
    "    T1, labels1 = first(network)\n",
    "    pos = 1\n",
    "    tree = ContractionTree(T1, pos, labels1)\n",
    "    for (T, labels) in network[2:5n-1]\n",
    "        pos += 1\n",
    "        nextleaf = ContractionTree(T, pos, labels)\n",
    "\n",
    "        labelsR = Tuple(symdiff(nextleaf.labels, tree.labels))\n",
    "     \n",
    "        labels1_to_labels2, labels1_to_labelsR, labels2_to_labelsR = contract_labels(nextleaf.labels, tree.labels, labelsR)\n",
    "        blocksR = [contract_blocks(b1, labels1_to_labelsR, b2, labels2_to_labelsR, ValLength(labelsR)) for b1 in nextleaf.blocks for b2 in tree.blocks]\n",
    "        indsR = contract_inds(nextleaf.inds, nextleaf.labels, tree.inds, tree.labels, labelsR)\n",
    "        \n",
    "        tree = ContractionTree(nextleaf, tree, 0, blocksR, indsR, labelsR)\n",
    "    end\n",
    "    return tree\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0cc9653c-cc21-4713-b4fc-d57dfb8673bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1)\n",
      "├─ (13, 11)\n",
      "└─ (13, 7, 11, 1)\n",
      "   ├─ (12, 9)\n",
      "   └─ (13, 7, 12, 11, 9, 1)\n",
      "      ├─ (10, 8)\n",
      "      └─ (13, 7, 12, 11, 10, 9, 8, 1)\n",
      "         ├─ (13, 7, 6)\n",
      "         └─ (12, 6, 11, 10, 9, 8, 1)\n",
      "            ├─ (12, 6, 5)\n",
      "            └─ (11, 5, 10, 9, 8, 1)\n",
      "               ├─ (11, 5, 4)\n",
      "               └─ (10, 4, 9, 8, 1)\n",
      "                  ├─ (10, 4, 3)\n",
      "                  └─ (9, 3, 8, 1)\n",
      "                     ├─ (9, 3, 2)\n",
      "                     └─ (8, 2, 1)\n",
      "FLOPs cost: 2.0054470536e10\n",
      "Peak memory: 2.0135236614e10\n"
     ]
    }
   ],
   "source": [
    "Δ = [BlockSparseTensor(ComplexF64, [(n, n) for n in 1:N_int], (ones(Int, N_int), ones(Int, N_int))) for i=1:3];\n",
    "tensor_network_wo_P = make_Σ_tensor_network_wo_P(top, O, Δ);\n",
    "\n",
    "tree = contraction_tree_orig_order(tensor_network_wo_P)\n",
    "print_tree(tree, maxdepth=20)\n",
    "println(\"FLOPs cost: \", Float64(flops_cost(tree)))\n",
    "println(\"Peak memory: \", Float64(peak_memory(tree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "11602572-aa59-4083-8d9f-4caea996d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1)\n",
      "├─ (13, 11)\n",
      "└─ (13, 7, 11, 1)\n",
      "   ├─ (12, 9)\n",
      "   └─ (13, 7, 12, 11, 9, 1)\n",
      "      ├─ (10, 8)\n",
      "      └─ (13, 7, 12, 11, 10, 9, 8, 1)\n",
      "         ├─ (13, 7, 6)\n",
      "         └─ (12, 6, 11, 10, 9, 8, 1)\n",
      "            ├─ (12, 6, 5)\n",
      "            └─ (11, 5, 10, 9, 8, 1)\n",
      "               ├─ (11, 5, 4)\n",
      "               └─ (10, 4, 9, 8, 1)\n",
      "                  ├─ (10, 4, 3)\n",
      "                  └─ (9, 3, 8, 1)\n",
      "                     ├─ (9, 3, 2)\n",
      "                     └─ (8, 2, 1)\n",
      "FLOPs cost: 3.9554812296e10\n",
      "Peak memory: 2.0135236644e10\n"
     ]
    }
   ],
   "source": [
    "Δ = [DiagBlockSparseTensor(ComplexF64(i), [Block(1, 1)], ([N_int], [N_int])) for i=1:3];\n",
    "tensor_network_wo_P = make_Σ_tensor_network_wo_P(top, O, Δ);\n",
    "\n",
    "tree = contraction_tree_orig_order(tensor_network_wo_P)\n",
    "print_tree(tree, maxdepth=20)\n",
    "println(\"FLOPs cost: \", Float64(flops_cost(tree)))\n",
    "println(\"Peak memory: \", Float64(peak_memory(tree)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
